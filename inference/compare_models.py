"""
Model Comparison Script
Compare the performance of different models on your dataset.
"""

import os
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np


def load_test_data():
    """Load test data for comparison."""
    print("üìÇ Loading test data...")
    
    # Use ground truth as test set
    gt_path = "data/groundTruthData/reviews_ground_truth.csv"
    if os.path.exists(gt_path):
        df = pd.read_csv(gt_path)
        texts = df['text'].tolist()
        labels = df['true_label'].tolist()
        print(f"‚úÖ Loaded {len(texts)} ground truth samples")
        return texts, labels
    
    # Fallback to policy validation data
    print("‚ö†Ô∏è Using policy validation data as test set")
    from datasets import load_from_disk
    
    try:
        val_dataset = load_from_disk("data/policy_val_tokenized")
        
        # Extract texts and labels from tokenized dataset
        texts = []
        labels = []
        
        # We need to get the original texts - let's load from policy filtered data
        policy_clean_path = "data/filteredData/cleaned_reviews_1756493203.csv"
        policy_flagged_path = "data/filteredDataWithFlags/cleaned_reviews_1756493203.csv"
        
        if os.path.exists(policy_clean_path) and os.path.exists(policy_flagged_path):
            # Load clean data (valid=1) 
            df_clean = pd.read_csv(policy_clean_path)
            clean_texts = df_clean['text'].tolist()[:50]  # Sample 50
            clean_labels = [1] * len(clean_texts)
            
            # Load flagged data (invalid=0)
            df_flagged = pd.read_csv(policy_flagged_path)
            flagged_texts = df_flagged['text'].tolist()[:50]  # Sample 50  
            flagged_labels = [0] * len(flagged_texts)
            
            texts = clean_texts + flagged_texts
            labels = clean_labels + flagged_labels
            
            print(f"‚úÖ Loaded {len(texts)} policy-based test samples")
            return texts, labels
        
        # Final fallback - use tokenized data with dummy texts
        for i, item in enumerate(val_dataset):
            if i < 100:  # Limit to 100 samples
                text = f"Test review sample {i+1}"  # Placeholder
                texts.append(text)
                labels.append(item['label'])
        
        print(f"‚ö†Ô∏è Using {len(texts)} placeholder test samples")
        return texts, labels
        
    except Exception as e:
        print(f"‚ùå Error loading test data: {e}")
        return [], []


def test_model(model_path, model_name, texts, true_labels):
    """Test a specific model on the given texts."""
    print(f"\nüß™ Testing {model_name}")
    print(f"üìÅ Model path: {model_path}")
    
    try:
        # Load model and tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        
        # Create pipeline
        classifier = pipeline(
            "text-classification",
            model=model,
            tokenizer=tokenizer,
            return_all_scores=True
        )
        
        # Make predictions
        print("üîÑ Making predictions...")
        predictions = []
        
        for i, text in enumerate(texts):
            if i % 50 == 0:
                print(f"   Progress: {i}/{len(texts)}")
            
            result = classifier(text)
            # Get label with highest score
            pred_label = 1 if result[0]['label'] == 'LABEL_1' else 0
            predictions.append(pred_label)
        
        # Calculate metrics
        accuracy = accuracy_score(true_labels, predictions)
        report = classification_report(true_labels, predictions, 
                                     target_names=['Invalid', 'Valid'], 
                                     output_dict=True)
        
        print(f"\nüìä {model_name} Results:")
        print(f"   Accuracy: {accuracy:.1%}")
        print(f"   Precision (Invalid): {report['Invalid']['precision']:.3f}")
        print(f"   Recall (Invalid): {report['Invalid']['recall']:.3f}")
        print(f"   Precision (Valid): {report['Valid']['precision']:.3f}")
        print(f"   Recall (Valid): {report['Valid']['recall']:.3f}")
        print(f"   F1-Score (Macro): {report['macro avg']['f1-score']:.3f}")
        
        return {
            'model': model_name,
            'accuracy': accuracy,
            'precision_invalid': report['Invalid']['precision'],
            'recall_invalid': report['Invalid']['recall'],
            'precision_valid': report['Valid']['precision'],
            'recall_valid': report['Valid']['recall'],
            'f1_macro': report['macro avg']['f1-score']
        }
        
    except Exception as e:
        print(f"‚ùå Error testing {model_name}: {e}")
        return None


def compare_models():
    """Compare different model versions."""
    print("üî¨ Model Performance Comparison")
    print("=" * 60)
    
    # Load test data
    texts, labels = load_test_data()
    
    # Models to test
    models_to_test = [
        ("./models/roberta_policy_based_model", "RoBERTa + Policy Data"),
        ("./models/final_model", "Previous Model (if exists)"),
    ]
    
    results = []
    
    for model_path, model_name in models_to_test:
        if os.path.exists(model_path):
            result = test_model(model_path, model_name, texts, labels)
            if result:
                results.append(result)
        else:
            print(f"‚ö†Ô∏è Model not found: {model_path}")
    
    # Display comparison
    if len(results) > 0:
        print(f"\nüìä Final Comparison Summary:")
        print("=" * 60)
        
        df_results = pd.DataFrame(results)
        print(df_results.to_string(index=False, float_format='%.3f'))
        
        # Find best model
        best_model = df_results.loc[df_results['accuracy'].idxmax()]
        print(f"\nüèÜ Best Performing Model: {best_model['model']}")
        print(f"   Best Accuracy: {best_model['accuracy']:.1%}")
        
        return df_results
    else:
        print("‚ùå No models found to compare")
        return None


if __name__ == "__main__":
    print("üöÄ Model Comparison Tool")
    print("This will test your models on ground truth data")
    print()
    
    results = compare_models()
    
    if results is not None:
        print(f"\nüí° Model Performance Tips:")
        print("- Accuracy > 80% = Excellent")
        print("- Accuracy 70-80% = Good") 
        print("- Accuracy 60-70% = Fair")
        print("- Accuracy < 60% = Needs improvement")
        print()
        print("üéØ Expected improvement: 25% ‚Üí 85%+ with RoBERTa + Policy data!")
